---
title: "REACT2023: REsponsible Affective CompuTing"
date: 2023-04-04T00:06:56+08:00
---

<br>
<div class="row">
  <div class="col-xs-12">
    <p><center>
    <img class="img-fluid" width="900" height="400" src="/2023/img/banner.jpg">
    <!-- <small style="float:right;margin-top:1mm;margin-right:12mm;">Image credit to <a href="https://pixabay.com/illustrations/eye-watercolor-art-sketch-4453129/" target="_blank">Nika_Akin</a></small> -->
    </center></p>
    <p><center>
      Full day, <font size="3" color="red">18th Oct 2023</font> (Full-day)
      <!-- <br><br>
      <b>7 papers</b> have been accepted in our GAZE2022 workshop.
      <br>
      Congratulations to all authors!
      <br><br>
      This year, June 19 and 20 marks Juneteenth, a US holiday commemorating the end of slavery in the US, and a holiday of special significance in the US South. We encourage attendees to learn more about Juneteenth and its historical context, and to join the city of New Orleans in celebrating the Juneteenth holiday. You can find out more information about Juneteenth here: <a href="https://cvpr2022.thecvf.com/recognizing-juneteenth" target="_blank">https://cvpr2022.thecvf.com/recognizing-juneteenth</a> -->
      <!-- Coming soon -->
      <!-- 3pm - 8:20pm UTC -->
      <!-- <table id="top-table">
        <style>
	  #top-table td {
	    padding: 1px 5px;
	  }
	  #top-table td:nth-child(1),
	  #top-table td:nth-child(3) {
	    text-align: right;
	    padding-right: 0px;
	  }
	</style>
        <tr><td>8am   </td><td>- 1:20pm     </td><td> PDT</td><td>(UTC-7)  </td></tr>
        <tr><td>11am  </td><td>- 4:20pm     </td><td> EDT</td><td>(UTC-4)  </td></tr>
        <tr><td>4pm   </td><td>- 9:20pm     </td><td> BST</td><td>(UTC+1)  </td></tr>
        <tr><td>5pm   </td><td>- 10:20pm    </td><td>CEST</td><td>(UTC+2)  </td></tr>
        <tr><td>8:30pm</td><td>- 1:50am (+1)</td><td> IST</td><td>(UTC+5.5)</td></tr>
        <tr><td>11pm  </td><td>- 4:20am (+1)</td><td> CST</td><td>(UTC+8)  </td></tr>
        <tr><td>12am  </td><td>- 5:20am (+1)</td><td> KST</td><td>(UTC+9)  </td></tr>
      </table> -->
      <!-- <br>
      Youtube recording: <a href="https://youtu.be/WQ8azMW_dn8" target="_blank">https://youtu.be/WQ8azMW_dn8</a> -->
    </center></p>
  </div>
</div><br>

<div class="row">
  <div class="col-xs-12"><a class="anchor" id="intro"></a>
    <h2>Introduction</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
With the rapid advancements in social computing, multimedia, and sensing technology, Affective Computing research have provoked more discussion about the potential consequences of intelligent AI equipped with emotional intelligence. Affective computing research is engaged with ethics at different stages, from training of emotionally intelligent models with enormous amount of human data to deploying the code in application specific environment. In principal, the development of any AI system must be guided by a concern for its human impact. The aim should be striving to augment and enhance humans, not replace human; while taking inspiration from human intelligence. The problem arises when in some areas especially for data protection and research with human participants; when explicit codes give the AI system legal force and it impact human implicitly/explicitly. To this end, Responsible AI address this issue by analyzing its potential implications and enhance technology in a privacy preserving way. The REACT 2023 workshop aims to transfer the same concept to small-scale, lab based environment to real-world, large-scale corpus enhanced with responsibility. The workshop also focuses to bring attention of the researchers and industry professionals on the potential implications of Emotional-AI developments, and evaluating the moral and ethical consequences. 

<div class="row">
  <div class="col-xs-12 panel-group"><a class="anchor" id="calls"></a>
    <h2>Call for Contributions</h2>
    <br>
    <div class="panel panel-default">
      <div class="panel-heading" data-toggle="collapse" data-parent="#call" href="#call-papers" style="cursor:pointer;">
        <h3 style="margin:0;">Full Workshop Papers</h3>
      </div>
      <div id="call-papers" class="panel-collapse collapse" data-parent="#call">
        <div class="panel-body">
          <p>
The 1st International Workshop on Responsible Affective computing (REACT 2023) at <a href="https://www.acmmm2023.org/" target="_blank">ACM-MM 2023</a> aims to encourage and highlight novel strategies for affective phenomena estimation and prediction with a focus on robustness and accuracy in extended parameter spaces, spatially, temporally, spatio-temporally and most importantly Responsibly. This is expected to be achieved by applying novel neural network architectures, incorporating anatomical insights and constraints, introducing new and challenging datasets, and exploiting multi-modal training. Specifically, the workshop topics include (but are not limited to):
    </p>
    <ul>
      <li>Privacy preserving large scale data collection and annotation for Affective Computing.</li>
      <li>Privacy preserving large scale emotion recognition in the wild.</li>
      <li>Responsible AI for emotion recognition.</li>
      <li>Privacy preserving fusion techniques for audio-visual/physiological signals.</li>
      <li>Privacy preserving  Localization and identification of salient affect signals.</li>
      <li>Affective Computing Applications in education, entertainment and healthcare.</li>
      <li>Privacy concerns in large scale data collection.</li>
      <li>Explainable AI in affective computing.</li>
      <li>Responsible Personalization of affective phenomena estimators with low data regime.</li>
      <li>Bias in affective computing data (e.g. lack of multi-cultural datasets).</li>
    </ul>
We will be hosting several invited speakers for the topic of Responsible affective computing. We will also be accepting the submission of full unpublished papers as done in previous versions of the workshop. These papers will be peer-reviewed via a double-blind process, and will be published in the official workshop proceedings and be presented at the workshop itself. 
  </div>
</div> <br>
	    <span style="font-weight:500;">Submission:</span> We invite authors to submit unpublished papers (8-page <a href="https://www.acmmm2023.org/cfp/" target="_blank">ACM MM format</a>) to our workshop, to be presented at a poster session upon acceptance. All submissions will go through a double-blind review process. All contributions must be submitted (along with supplementary materials, if any) at the CMT link.
	    <!--<a href="https://cmt3.research.microsoft.com/REACT2023/Submission/Index">CMT link</a>.-->
	    <!-- <a href="https://cmt3.research.microsoft.com/GAZE2021/Submission/Index" target="_blank" title="CMT Submission System for GAZE 2021">this CMT link</a>. -->
	  </p>
	  <p>
	    Accepted papers will be published in the official CVPR Workshops proceedings and the Computer Vision Foundation (CVF) Open Access archive.
	  </p>
	  <p>
	    <span style="font-weight:500;">Note:</span> Authors of previously rejected main conference submissions are also welcome to submit their work to our workshop. When doing so, you must submit the previous reviewers' comments (named as <code>previous_reviews.pdf</code>) and a letter of changes (named as <code>letter_of_changes.pdf</code>) as part of your supplementary materials to clearly demonstrate the changes made to address the comments made by previous reviewers.
	    <!--Due to potential clashes with the main conference reviewing schedule, we will accept simultaneous submissions to the ICCV main conference and GAZE Workshop. Simultaneous submissions are otherwise disallowed.-->
          </p>
        </div>
      </div>
    </div>
    <br>
       
  </div>
</div><br>

<div class="row">
  <div class="col-xs-12"><a class="anchor" id="dates"></a>
    <h2>Important Dates</h2>
    <br>
    <table class="table table-striped">
      <tbody>
        <!-- <tr>
          <td>ETH-XGaze &amp; EVE Challenges Released</td>
          <td>February 13, 2021</td>
        </tr> -->
        <tr>
          <td>Paper Submission Deadline</td>
          <td>June 20, 2023 (12:00 Pacific time)</td>
	  <td><span class="countdown" reference="13 Mar 2023 12:00:00 PST"></span></td>
        </tr>
        <tr>
          <td>Notification to Authors</td>
          <td>July 20, 2023</td>
        </tr>
        <tr>
          <td>Camera-Ready Deadline</td>
          <td>July 30, 2023</td>
        </tr>
        <!-- <tr>
          <td>ETH-XGaze &amp; EVE Challenges Closed</td>
          <td>May 28, 2021 (23:59 UTC)</td>
	  <td><span class="countdown" reference="28 May 2022 23:59:59 UTC"></span></td>
        </tr> -->
      </tbody>
    </table>
  </div>
</div><br>

<div class="row">
  <div class="col-xs-12"><a class="anchor" id="schedule"></a>
     <h2>Workshop Schedule</h2>
    TBD
     <!-- <p>
       Attending:
       <ul>
         <li>Registered ACMMM attendees can find the relevant Zoom and Gatherly links at <a target="_blank" href="https://www.eventscribe.net/2021/2021CVPR/login.asp">https://www.eventscribe.net/2021/2021CVPR/login.asp</a></li>
	 <li>Others are welcome to join our livestream at <a href="https://youtu.be/ScoHuri_3hs">https://youtu.be/ScoHuri_3hs</a></li>
       </ul>
     </p> -->
     <!-- <table class="table schedule" style="border:none !important;">
      <thead class="thead-light">
        <tr>
	  <th>Time in UTC</th>
	  <th>Start Time in UTC<span class="tz-offset"></span><b>*</b><br><span class="tz-subtext">(probably your time zone)</span></th>
          <th>Item</th>
        </tr>
      </thead>
      <tbody>-->
        <!-- <tr>
          <td>1:30pm - 1:35pm</td>
          <td class="to-local-time">20 Jun 2022 13:30:00 UTC</td>
          <td>Opening remark</td>
        </tr>
        <tr>
          <td>1:35pm - 2:15pm</td>
          <td class="to-local-time">20 Jun 2022 13:35:00 UTC</td>
          <td>Invited talk by Prof. Wei Shen</td>
        </tr>
        <tr>
          <td>2:15pm - 2:55pm</td>
          <td class="to-local-time">20 Jun 2022 14:15:00 UTC</td>
          <td>Invited talk by Prof. Gordon Wetzstein</td>
        </tr>
        <tr>
          <td>2:55pm - 3:00pm</td>
          <td class="to-local-time">20 Jun 2022 14:55:00 UTC</td>
          <td>Invited poster spotlight talk</td>
        </tr>
        <tr>
          <td>3:00pm - 4:00pm</td>
          <td class="to-local-time">20 Jun 2022 15:00:00 UTC</td>
          <td>Coffee break & poster presentation</td>
        </tr>
        <tr>
          <td>4:00pm - 5:10pm</td>
          <td class="to-local-time">20 Jun 2022 16:00:00 UTC</td>
          <td>Workshop paper presentation</td>
        </tr>
        <tr>
          <td>5:10pm - 5:50pm</td>
          <td class="to-local-time">20 Jun 2022 17:10:00 UTC</td>
          <td>Panel discussion</td>
        </tr>
        <tr>
          <td>5:50pm - 6:00pm</td>
          <td class="to-local-time">20 Jun 2022 17:50:00 UTC</td>
          <td>Award & closing remark</td>
        </tr> -->
        <!-- <tr>
          <td>8:15pm - 8:20pm</td>
          <td class="to-local-time">20 Jun 2021 20:15:00 UTC</td>
          <td>Award & closing remark</td>
        </tr> -->
      <!-- </tbody>
     </table>
     <span class="disclaimer">
     * This time is calculated to be in your computer's reported time zone.
     <br>
     For example, those in Los Angeles may see UTC-7,
     <br>
     while those in Berlin may see UTC+2.
     <br>
     <br>
     Please note that there may be differences to your actual time zone.</span>
  </div>
</div><br>-->


<div class="row">
  <div class="col-xs-12"><a class="anchor" id="speakers"></a>
    <h2>Invited Keynote Speakers</h2>
	<br>  

  <div class="row speaker">
      <div class="col-sm-3 speaker-pic">
        <a href="">
          <img class="people-pic" src="/2023/img/people/jh.jpg" />
        </a>
        <div class="people-name">
          <a href="">Jessey Hoey</a>
          <h6>University of Waterloo</h6>
        </div>
      </div>
      <div class="col-sm-9">
        <div class="panel panel-default">
          <div class="panel-heading" data-toggle="collapse" href="#jr-bio" style="cursor:pointer;text-align:center">
            <b>Biography <span style="font-weight:normal">(click to expand/collapse)</span></b>
          </div>
          <div id="jr-bio" class="panel-collapse collapse"><div class="panel-body">
            <p class="speaker-bio">
           Prof. Jesse Hoey is a professor in the David R. Cheriton School of Computer Science at the University of Waterloo, where he leads the Computational Health Informatics Laboratory (CHIL). He is a Faculty Affiliate at the Vector Institute, and an affiliate scientist at KITE/TRI, both in Toronto. Dr. Hoey holds a Ph.D degree (2004) in computer science from the University of British Columbia. He has published over one hundred peer reviewed scientific papers. His primary research interest is to understand the nature of human emotional intelligence by attempting to build computational models of some of its core functions, and to apply them in domains with social and economic impact. He is an associate editor for the IEEE Transactions on Affective Computing.
            </p>
          </div></div>
        </div>
      </div>
    </div>
    <br>

  
  <br>

<div class="row">
  <div class="col-xs-12"><a class="anchor" id="awards"></a>
    <h2>Awards</h2>
    TBD

  <!-- <div class="award">
      <h3>
	    <b> Best Paper Award </b>
	    <span class="award-sponsor">sponsored by
        <a href="https://www.nvidia.com/" target="_blank"><img src="img/nvidia.jpg" /></a>
	    </span>
      </h3>
      <p><br>
	Learning-by-Novel-View-Synthesis for Full-Face Appearance-Based 3D Gaze Estimation<br>
	<i>Jiawei Qin, Takuru Shimoyama, Yusuke Sugano</i>
        <div class="btn-group btn-group-xs" role="group">
          <button class="btn btn-success">GAZE 2022</button>
	        <a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2022W/GAZE/papers/Qin_Learning-by-Novel-View-Synthesis_for_Full-Face_Appearance-Based_3D_Gaze_Estimation_CVPRW_2022_paper.pdf"><i class="fas fa-file-pdf"></i> PDF (CVF)</a>
	        <a class="btn btn-default" target="_blank" href="https://youtu.be/BUFTzo5DqXc"><i class="fas fa-video"></i> Video</a>
        </div>
      </p>
    </div>

  <div class="award">
      <h3>
	    <b> Best Poster Award </b>
	    <span class="award-sponsor">sponsored by
        <a href="https://www.google.com/" target="_blank"><img src="img/google.png" /></a>
	    </span>
      </h3>
      <p><br>
	Unsupervised Multi-View Gaze Representation Learning<br>
	<i>John Gideon, Shan Su, Simon Stent</i>
        <div class="btn-group btn-group-xs" role="group">
          <button class="btn btn-success">GAZE 2022</button>
	        <a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2022W/GAZE/papers/Gideon_Unsupervised_Multi-View_Gaze_Representation_Learning_CVPRW_2022_paper.pdf"><i class="fas fa-file-pdf"></i> PDF (CVF)</a>
	        <a class="btn btn-default" target="_blank" href="https://youtu.be/W0OK1vVtiEk"><i class="fas fa-video"></i> Video</a>
        </div>
      </p>
    </div>
  </div>
</div><br>

   <br><br>

<div class="row">
  <div class="col-xs-12"><a class="anchor" id="accepted-papers"></a>
    <h2>Accepted Full Papers</h2>

  <div class="paper">
        <span class="title">Learning-by-Novel-View-Synthesis for Full-Face Appearance-Based 3D Gaze Estimation</span>
        <span class="authors">Jiawei Qin, Takuru Shimoyama, Yusuke Sugano</span>
        <span class="award">Best Paper Award</span>
        <div class="btn-group btn-group-xs" role="group">
          <button class="btn btn-success">GAZE 2022</button>
	  <a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2022W/GAZE/papers/Qin_Learning-by-Novel-View-Synthesis_for_Full-Face_Appearance-Based_3D_Gaze_Estimation_CVPRW_2022_paper.pdf"><i class="fas fa-file-pdf"></i> PDF (CVF)</a>
	  <a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2022W/GAZE/supplemental/Qin_Learning-by-Novel-View-Synthesis_for_Full-Face_CVPRW_2022_supplemental.pdf"><i class="fas fa-file-pdf"></i> Suppl. (CVF)</a>
	  <a class="btn btn-default" target="_blank" href="http://arxiv.org/abs/2201.07927"><i class="fas fa-archive"></i> arXiv</a>
      <a class="btn btn-default" target="_blank" href="https://youtu.be/BUFTzo5DqXc"><i class="fas fa-video"></i> Video</a>
        </div>
    </div>

  <div class="paper">
        <span class="title">Self-Attention with Convolution and Deconvolution for Efficient Eye Gaze Estimation from a Full Face Image</span>
        <span class="authors">Jun O Oh, Hyung Jin Chang, Sang-Il Choi</span>
        <div class="btn-group btn-group-xs" role="group">
          <button class="btn btn-success">GAZE 2022</button>
	  <a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2022W/GAZE/papers/Oh_Self-Attention_With_Convolution_and_Deconvolution_for_Efficient_Eye_Gaze_Estimation_CVPRW_2022_paper.pdf"><i class="fas fa-file-pdf"></i> PDF (CVF)</a>
      <a class="btn btn-default" target="_blank" href="https://youtu.be/ANQ65NNNWNE"><i class="fas fa-video"></i> Video</a>
        </div>
    </div>

  <div class="paper">
        <span class="title">Unsupervised Multi-View Gaze Representation Learning</span>
        <span class="authors">John Gideon, Shan Su, Simon Stent</span>
        <span class="award">Best Poster Award</span>
        <div class="btn-group btn-group-xs" role="group">
          <button class="btn btn-success">GAZE 2022</button>
	  <a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2022W/GAZE/papers/Gideon_Unsupervised_Multi-View_Gaze_Representation_Learning_CVPRW_2022_paper.pdf"><i class="fas fa-file-pdf"></i> PDF (CVF)</a>
      <a class="btn btn-default" target="_blank" href="https://youtu.be/W0OK1vVtiEk"><i class="fas fa-video"></i> Video</a>
        </div>
    </div>

  <div class="paper">
        <span class="title">ScanpathNet: A Recurrent Mixture Density Network for Scanpath Prediction</span>
        <span class="authors">Ryan Anthony J de Belen, Tomasz Bednarz, Arcot Sowmya</span>
        <span class="award">Best Paper Honourable Mention</span>
        <div class="btn-group btn-group-xs" role="group">
          <button class="btn btn-success">GAZE 2022</button>
	  <a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2022W/GAZE/papers/de_Belen_ScanpathNet_A_Recurrent_Mixture_Density_Network_for_Scanpath_Prediction_CVPRW_2022_paper.pdf"><i class="fas fa-file-pdf"></i> PDF (CVF)</a>
	  <a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2022W/GAZE/supplemental/de_Belen_ScanpathNet_A_Recurrent_CVPRW_2022_supplemental.pdf"><i class="fas fa-file-pdf"></i> Suppl. (CVF)</a>
      <a class="btn btn-default" target="_blank" href="https://youtu.be/8RXog3XkCl8"><i class="fas fa-video"></i> Video</a>
        </div>
    </div>

  <div class="paper">
        <span class="title">One-Stage Object Referring with Gaze Estimation</span>
        <span class="authors">Jianhang Chen, Xu Zhang, Yue Wu, Shalini Ghosh, Pradeep Natarajan, Shih-Fu Chang, Jan Allebach</span>
        <div class="btn-group btn-group-xs" role="group">
          <button class="btn btn-success">GAZE 2022</button>
	  <a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2022W/GAZE/papers/Chen_One-Stage_Object_Referring_With_Gaze_Estimation_CVPRW_2022_paper.pdf"><i class="fas fa-file-pdf"></i> PDF (CVF)</a>
      <a class="btn btn-default" target="_blank" href="https://youtu.be/SkjtCXX-aJY"><i class="fas fa-video"></i> Video</a>
        </div>
    </div>

  <div class="paper">
        <span class="title">Characterizing Target-absent Human Attention</span>
        <span class="authors">Yupei Chen, Zhibo Yang, Souradeep Chakraborty, Sounak Mondal, Seoyoung Ahn, Dimitris Samaras, Minh Hoai, Gregory Zelinsky</span>
        <div class="btn-group btn-group-xs" role="group">
          <button class="btn btn-success">GAZE 2022</button>
	  <a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2022W/GAZE/papers/Chen_Characterizing_Target-Absent_Human_Attention_CVPRW_2022_paper.pdf"><i class="fas fa-file-pdf"></i> PDF (CVF)</a>
	  <a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2022W/GAZE/supplemental/Chen_Characterizing_Target-Absent_Human_CVPRW_2022_supplemental.pdf"><i class="fas fa-file-pdf"></i> Suppl. (CVF)</a>
      <a class="btn btn-default" target="_blank" href="https://youtu.be/SIVywYz2pNs"><i class="fas fa-video"></i> Video</a>
        </div>
    </div>

  <div class="paper">
        <span class="title">A Modular Multimodal Architecture for Gaze Target Prediction: Application to Privacy-Sensitive Settings</span>
        <span class="authors">Anshul Gupta, Samy Tafasca, Jean-Marc Odobez</span>
        <div class="btn-group btn-group-xs" role="group">
          <button class="btn btn-success">GAZE 2022</button>
	  <a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2022W/GAZE/papers/Gupta_A_Modular_Multimodal_Architecture_for_Gaze_Target_Prediction_Application_to_CVPRW_2022_paper.pdf"><i class="fas fa-file-pdf"></i> PDF (CVF)</a>
	  <a class="btn btn-default" target="_blank" href="https://youtu.be/z-XSwLOpNzw"><i class="fas fa-video"></i> Video</a>
        </div>
    </div>
  <br><br>

  <a class="anchor" id="invited-posters"></a>
    <h2>Invited Posters</h2>

  <div class="paper">
        <span class="title">Dynamic 3D Gaze from Afar: Deep Gaze Estimation from Temporal Eye-Head-Body Coordination</span>
        <span class="authors">Soma Nonaka, Shohei Nobuhara, Ko Nishino</span>
        <div class="btn-group btn-group-xs" role="group">
          <button class="btn btn-primary">CVPR 2022</button>
	  <a class="btn btn-default" target="_blank" href="https://vision.ist.i.kyoto-u.ac.jp/pubs/SNonaka_CVPR22.pdf"><i class="fas fa-file-pdf"></i> PDF</a>
	  <a class="btn btn-default" target="_blank" href="https://vision.ist.i.kyoto-u.ac.jp/pubs/SNonaka_CVPR22_supp.pdf"><i class="fas fa-file-pdf"></i> Supp.</a>
          <a class="btn btn-default" target="_blank" href="https://github.com/kyotovision-public/dynamic-3d-gaze-from-afar"><i class="fas fa-code"></i> Code</a>
	  <a class="btn btn-default" target="_blank" href="https://youtu.be/IEc8E4e4mXU"><i class="fas fa-video"></i> Video</a>
        </div>
    </div> -->

  <!-- <div class="paper">
        <span class="title">Dual Attention Guided Gaze Target Detection in the Wild</span>
        <span class="authors">Yi Fang, Jiapeng Tang, Wang Shen, Wei Shen, Xiao Gu, Li Song, and Guangtao Zhai</span>
        <div class="btn-group btn-group-xs" role="group">
          <button class="btn btn-primary">CVPR 2021 (Oral)</button>
          <button class="btn btn-poster-id">Poster #8</button>
	  <a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2021/papers/Fang_Dual_Attention_Guided_Gaze_Target_Detection_in_the_Wild_CVPR_2021_paper.pdf"><i class="fas fa-file-pdf"></i> PDF (CVF)</a>
          <a class="btn btn-default" target="_blank" href="https://arxiv.org/abs/1910.07331"><i class="fas fa-archive"></i> arXiv</a>
          <a class="btn btn-default" target="_blank" href="https://github.com/NVlabs/few_shot_gaze"><i class="fas fa-code"></i> Code</a>
        </div>
    </div>

  <div class="paper">
        <span class="title">Connecting What To Say With Where To Look by Modeling Human Attention Traces</span>
        <span class="authors">Zihang Meng, Licheng Yu, Ning Zhang, Tamara L. Berg, Babak Damavandi, Vikas Singh, and Amy Bearman</span>
        <div class="btn-group btn-group-xs" role="group">
          <button class="btn btn-primary">CVPR 2021</button>
          <button class="btn btn-poster-id">Poster #9</button>
	  <a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2021/papers/Meng_Connecting_What_To_Say_With_Where_To_Look_by_Modeling_CVPR_2021_paper.pdf"><i class="fas fa-file-pdf"></i> PDF (CVF)</a>
	  <a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2021/supplemental/Meng_Connecting_What_To_CVPR_2021_supplemental.pdf"><i class="fas fa-file-pdf"></i> Supp. (CVF)</a>
	  <a class="btn btn-default" target="_blank" href="https://arxiv.org/abs/2105.05964"><i class="fas fa-archive"></i> arXiv</a>
	  <a class="btn btn-default" target="_blank" href="https://github.com/facebookresearch/connect-caption-and-trace"><i class="fas fa-code"></i> Code</a>
        </div>
    </div>

  <div class="paper">
        <span class="title">ETH-XGaze: A Large Scale Dataset for Gaze Estimation under Extreme Head Pose and Gaze Variation</span>
        <span class="authors">Xucong Zhang, Seonwook Park, Thabo Beeler, Derek Bradley, Siyu Tang, and Otmar Hilliges</span>
        <div class="btn-group btn-group-xs" role="group">
          <button class="btn btn-primary">ECCV 2020 (Spotlight)</button>
          <button class="btn btn-poster-id">Poster #10</button>
	  <a class="btn btn-default" target="_blank" href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123500358.pdf"><i class="fas fa-file-pdf"></i> PDF (ECVA Open Access)</a>
          <a class="btn btn-default" target="_blank" href="https://ait.ethz.ch/projects/2020/ETH-XGaze/"><i class="fas fa-atom"></i> Project Page</a>
          <a class="btn btn-default" target="_blank" href="https://arxiv.org/abs/1910.07331"><i class="fas fa-archive"></i> arXiv</a>
          <a class="btn btn-default" target="_blank" href="https://github.com/NVlabs/few_shot_gaze"><i class="fas fa-code"></i> Code</a>
        </div>
    </div>

  <div class="paper">
        <span class="title">Towards End-to-end Video-based Eye-Tracking</span>
        <span class="authors">Seonwook Park, Emre Aksan, Xucong Zhang, and Otmar Hilliges</span>
        <div class="btn-group btn-group-xs" role="group">
          <button class="btn btn-primary">ECCV 2020</button>
          <button class="btn btn-poster-id">Poster #11</button>
	  <a class="btn btn-default" target="_blank" href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123570732.pdf"><i class="fas fa-file-pdf"></i> PDF (ECVA Open Access)</a>
          <a class="btn btn-default" target="_blank" href="https://ait.ethz.ch/projects/2020/EVE/"><i class="fas fa-atom"></i> Project Page</a>
          <a class="btn btn-default" target="_blank" href="https://arxiv.org/abs/1910.07331"><i class="fas fa-archive"></i> arXiv</a>
          <a class="btn btn-default" target="_blank" href="https://github.com/NVlabs/few_shot_gaze"><i class="fas fa-code"></i> Code</a>
        </div>
    </div> -->


  </div>
</div>
<br><br>
, 


<div class="row" id="programcommittee">
  <div class="col-xs-12">
    <h2>Program Committee</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-3">
    <div class="people-name"><a target="_blank" href="">Karan Sikka</a><h6>SRI Inc</h6></div>
    <div class="people-name"><a target="_blank" href="">Fabien Ringeval</a><h6>University of Grenoble</h6></div>
    <div class="people-name"><a target="_blank" href="">Lei Chen</a><h6>ETS</h6></div>
    <div class="people-name"><a target="_blank" href="">Cha Zhang</a><h6>Microsoft Research</h6></div>
  </div>
  <div class="col-xs-3">
    <div class="people-name"><a target="_blank" href="">Laszlo A. Jeni</a><h6>Carnegie Mellon University</h6></div>
    <div class="people-name"><a target="_blank" href="">Markus Kachele</a><h6>University of Ulm</h6></div>
    <div class="people-name"><a target="_blank" href="">Akshay Asthana</a><h6>Seeing Machines Inc</h6></div>
    <div class="people-name"><a target="_blank" href="">Tadas Baltrusaitis </a><h6>Carnegie Mellon University</h6></div>
  </div>
  <div class="col-xs-3">
    <div class="people-name"><a target="_blank" href="">Xiaohua Huang </a><h6>University of Oulu</h6></div>
    <div class="people-name"><a target="_blank" href="">Veronica Perez</a><h6>University of Michigan</h6></div>
    <div class="people-name"><a target="_blank" href="">Bo Xiao</a><h6>Amazon Inc</h6></div>
    <div class="people-name"><a target="_blank" href="">Vitomir Struc </a><h6>University of Ljubljana</h6></div>
  </div>
  <div class="col-xs-3">
    <div class="people-name"><a target="_blank" href="https://sites.google.com/site/usmantariq">Usman Tariq</a><h6>American University of Sharjah</h6></div>
    <div class="people-name"><a target="_blank" href="https://engineering.tamu.edu/cse/profiles/chaspari-theodora.html">Theodora Chaspari</a><h6>Texas A&M University</h6></div>
    <div class="people-name"><a target="_blank" href="https://profiles.ucsf.edu/sanjay.ghosh">Sanjay Ghosh</a><h6>University of California San Francisco</h6></div>
    <div class="people-name"><a target="_blank" href="https://www.linkedin.com/in/zhixi-cai-b5b042259/?originalSubdomain=au">Zhixi Cai</a><h6>Monash University</h6></div>
    <div class="people-name"><a target="_blank" href="https://scholar.google.co.in/citations?user=_gpSvvwAAAAJ&hl=en">Garima Sharma</a><h6>Monash University</h6></div>
  </div>
</div>
<br>

<br>

<div class="row">
  <div class="col-xs-12"><a class="anchor" id="organizers"></a>
    <h2>Organizers</h2>
  </div>
</div>

<br><br>

<div class="row">
  <div class="col-xs-1"></div>
  <div class="col-xs-2">
    <a href="https://staffportal.curtin.edu.au/staff/profile/view/shreya-ghosh-a2f9d3ca/">
      <img class="people-pic" src="/2023/img/people/ShreyaGhosh.jpg"/> 
    </a>
    <div class="people-name">
      <a href="https://staffportal.curtin.edu.au/staff/profile/view/shreya-ghosh-a2f9d3ca/">Shreya Ghosh</a>
      <h6>Curtin University</h6>
    </div>
  </div>
  <div class="col-xs-2">
    <a href="https://iitrpr.ac.in/cse/abhinavdhall">
      <img class="people-pic" src="/2023/img/people/AbhinavDhall.jpg">
    </a>
    <div class="people-name">
      <a href="https://iitrpr.ac.in/cse/abhinavdhall">Abhinav Dhall</a>
      <h6>IIT Ropar</h6>
    </div>
  </div>
 
  <div class="col-xs-2">
    <a href="http://eecs.qmul.ac.uk/people/profiles/kolliasdimitrios.html">
      <img class="people-pic" src="/2023/img/people/DimitriosKollias.jpg">
    </a>
    <div class="people-name">
      <a href="http://eecs.qmul.ac.uk/people/profiles/kolliasdimitrios.html">Dimitrios Kollias</a>
      <h6>Queen Mary University of London</h6>
    </div>
  </div>
  <div class="col-xs-2">
    <a href="https://researchprofiles.canberra.edu.au/en/persons/roland-goecke">
      <img class="people-pic" src="/2023/img/people/RolandGoetcke.jpg">
    </a>
    <div class="people-name">
      <a href="https://researchprofiles.canberra.edu.au/en/persons/roland-goecke">Roland Goetcke</a>
      <h6>University of Canberra</h6>
    </div>
  </div>
  <div class="col-xs-2">
    <a href="https://staffportal.curtin.edu.au/staff/profile/view/tom-gedeon-5e48a1fd/">
      <img class="people-pic" src="/2023/img/people/TomGedeon.jpg">
    </a>
    <div class="people-name">
      <a href="https://staffportal.curtin.edu.au/staff/profile/view/tom-gedeon-5e48a1fd/">Tom Gedeon</a>
      <h6>Curtin University</h6>
    </div>
  </div>
  <div class="col-xs-1"></div>
</div>
<br>
<div class="row">
  <div class="col-xs-1"></div>
  <!-- <div class="col-xs-2">
    <a href="https://www.ecse.rpi.edu/~qji/">
      <img class="people-pic" src="{{ "img/people/qj.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://www.ecse.rpi.edu/~qji/">Qiang Ji</a>
      <h6>Rensselaer Polytechnic Institute</h6>
    </div>
  </div> -->
  <!-- <div class="col-xs-2">
    <a href="https://ait.ethz.ch/people/hilliges/">
      <img class="people-pic" src="{{ "img/people/oh.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://ait.ethz.ch/people/hilliges/">Otmar Hilliges</a>
      <h6>ETH Zürich</h6>
    </div>
  </div>
  <div class="col-xs-2">
    <a href="https://www.cs.bham.ac.uk/~leonarda/">
      <img class="people-pic" src="{{ "img/people/al.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://www.cs.bham.ac.uk/~leonarda/">Aleš Leonardis</a>
      <h6>University of Birmingham</h6>
    </div>
  </div>
</div><br> -->

<div class="row">
  <div class="col-xs-12"><a class="anchor" id="organizers"></a>
    <h2>Website Chair</h2>
  </div>
</div>

<br><br>

<div class="row">
  <div class="col-xs-1"></div>
  <div class="col-xs-2">
    <a href="[https://hengfei-wang.github.io//github.io/](https://staffportal.curtin.edu.au/staff/profile/view/shreya-ghosh-a2f9d3ca/)">
      <img class="people-pic" src="img/people/ShreyaGhosh.jpg">
    </a>
    <div class="people-name">
      <a href="https://staffportal.curtin.edu.au/staff/profile/view/shreya-ghosh-a2f9d3ca/">Shreya Ghosh</a>
      <h6>Curtin University</h6>
    </div>
	  <a href="[https://hengfei-wang.github.io//github.io/](https://staffportal.curtin.edu.au/staff/profile/view/shreya-ghosh-a2f9d3ca/)">
      <img class="people-pic" src="">
    </a>
    <div class="people-name">
      <a href="https://staffportal.curtin.edu.au/staff/profile/view/shreya-ghosh-a2f9d3ca/">Zhixi Cai</a>
      <h6>Monash University</h6>
    </div>
  </div>
  <br><br>
  <!-- <div class="col-xs-2"> -->
    Please contact me if you have any question about this website.
    <br>
    Email: shreya.ghosh@curtin.edu.au
  <!-- </div> -->
</div>
<br>


<!-- <div class="row">
  <div class="col-xs-12"><a class="anchor" id="sponsors"></a>
    <h2>Workshop sponsored by:</h2>
  </div>
</div> -->

<!-- <div class="row">
  <div class="col-xs-4 sponsor">
    <a href="https://www.nvidia.com/"><img src="img/nvidia.jpg" /></a>
  </div>
  <div class="col-xs-4 sponsor">
    <a href="https://www.google.com/"><img src="img/google.png" /></a>
  </div>
   <div class="col-xs-4 sponsor">
    <a href="https://www.tobii.com/"><img src="img/tobii.jpg" /></a>
  </div>
</div> -->